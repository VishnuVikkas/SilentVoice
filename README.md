SilentVoice is a comprehensive system for American Sign Language (ASL) recognition and interpretation, combining camera-based gesture input with AI-powered language translation and speech output.
The system consists of two main components: 
  1. a model trainer for collecting and labeling hand sign data using computer vision (Mediapipe and OpenCV)
  2. and a live interpreter application that uses a trained neural network (MLPClassifier) to recognize ASL gestures, predict words, and translate them into multiple languages with speech synthesis.

 The interface offers real-time word suggestions, direct text input, and user-friendly features such as accuracy feedback, language selection, and dynamic translation, making it accessible for both data collection and interactive real-time sign language interpretation. 

What SilentVoice Does?
1. Communication is one of the most essential aspects of human interaction. However, individuals with speech impairments often face difficulties in expressing themselves to people who are not familiar with sign        language. To bridge this communication gap, the SilentVoice â€“ ASL Translator project aims to translate American Sign Language (ASL) gestures into text and speech in real-time.
2. This project leverages computer vision and machine learning to recognize hand gestures using a webcam feed, processes them through trained models, and converts the identified gestures into meaningful words or       sentences. It also integrates translation and speech synthesis to make communication smoother and more inclusive.
   
By combining technologies like MediaPipe, OpenCV, and Natural Language Processing (NLP), the system provides a practical and accessible tool that promotes inclusivity and empowers individuals with speech to communicate effortlessly with others.
